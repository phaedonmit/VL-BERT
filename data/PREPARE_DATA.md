# Prepare Data

Download datasets as you need, and organize them as following:
 ```
code_root/
└── data/
    ├── conceptual-captions/
    │   ├── train_image/
    │   ├── val_image/
    │   ├── train_frcnn/
    │   ├── val_frcnn/
    │   ├── train.json
    │   ├── val.json
    │   ├── train_frcnn.json
    │   └── val_frcnn.json
    ├── en_corpus/
    │   ├── wiki.doc
    │   └── bc1g.doc
    ├── flickr30k/
    │   ├── train_image/
    |   ├── val_image/
    |   ├── test_image/
    |   ├── test_image2018/
    │   ├── train_image_frcnn/
    |   ├── val_image_frcnn/
    |   ├── test_image_frcnn/
    |   ├── test_image2018_frcnn/    
    │   ├── train.json
    │   ├── val.json
    │   ├── test.json
    │   ├── test2018.json    
    │   ├── train_frcnn.json
    │   └── val_frcnn.json
    │   ├── test_frcnn.json
    │   └── test2018_frcnn.json    
    │   ├── train_MLT_frcnn.json
    │   └── val_MLT_frcnn.json
    │   ├── test_MLT_frcnn.json    
    ├── WMT14/    
        ├── train.json
        ├── test.json
        
 ```
## Training Data

### Conceptual Captions
See [ReadMe.txt](./conceptual-captions/ReadMe.txt).

### English Wikipedia & BooksCorpus
* Wikipedia: [GoogleDrive](https://drive.google.com/file/d/1rZJ-Nj_SSqwu85tME3wbN8tfGhljfAsf/view?usp=sharing) / [BaiduPan](https://pan.baidu.com/s/1HSgUZXRESxVnx9ATOHwSrQ)
* BooksCorpus: [GoogleDrive](https://drive.google.com/file/d/16T5EYqIjO-tAj1OFxz6bnnzEABCusCcv/view?usp=sharing) / [BaiduPan](https://pan.baidu.com/s/1797WFFUTnRJakgGxefSrBg)

### Multi30k
* Multi30k dataset: [Github](https://github.com/multi30k/dataset)
* See [README file](https://github.com/phaedonmit/VL-BERT/blob/master/data/flickr30k/README.md)

### WMT'14
* WMT'14 dataset: [Stanford Natural Language Processing Group](https://nlp.stanford.edu/projects/nmt/)

